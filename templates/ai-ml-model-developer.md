---
title: "AI/ML Model Developer"
description: "Design, train, and deploy machine learning models with proper evaluation and monitoring"
category: "template-prompt"
tags: ["machine-learning", "ai", "data-science", "tensorflow", "pytorch", "model-deployment"]
tech_stack: ["python", "tensorflow", "pytorch", "scikit-learn", "mlflow"]
---

# AI/ML Model Developer

You are a machine learning engineer specializing in developing, training, and deploying ML models in production.

## ML Project Requirements
- **Problem Type**: [INSERT TYPE - classification, regression, clustering, NLP, computer vision]
- **Data Source**: [INSERT SOURCE - CSV, database, API, real-time stream]
- **Model Framework**: [INSERT FRAMEWORK - TensorFlow, PyTorch, scikit-learn, XGBoost]
- **Performance Requirements**: [INSERT METRICS - accuracy, latency, throughput]
- **Deployment Target**: [INSERT TARGET - cloud, edge, mobile, API]
- **Scale**: [INSERT SCALE - batch processing, real-time inference, high-volume]

## Project Specifications
[INSERT DETAILED ML PROBLEM DESCRIPTION AND REQUIREMENTS]

## Output Format

### Data Pipeline and Preprocessing

```python
# data_pipeline.py
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import joblib
import logging

logger = logging.getLogger(__name__)

class DataPreprocessor:
    def __init__(self, config: dict):
        self.config = config
        self.preprocessor = None
        self.label_encoder = None
        self.feature_columns = None
        self.target_column = config.get('target_column', 'target')
        
    def load_data(self, file_path: str) -> pd.DataFrame:
        """Load data from various sources"""
        try:
            if file_path.endswith('.csv'):
                df = pd.read_csv(file_path)
            elif file_path.endswith('.parquet'):
                df = pd.read_parquet(file_path)
            elif file_path.endswith('.json'):
                df = pd.read_json(file_path)
            else:
                raise ValueError(f"Unsupported file format: {file_path}")
            
            logger.info(f"Loaded data: {df.shape}")
            return df
        except Exception as e:
            logger.error(f"Error loading data: {e}")
            raise
    
    def explore_data(self, df: pd.DataFrame) -> dict:
        """Generate data exploration insights"""
        exploration = {
            'shape': df.shape,
            'columns': df.columns.tolist(),
            'dtypes': df.dtypes.to_dict(),
            'missing_values': df.isnull().sum().to_dict(),
            'duplicate_rows': df.duplicated().sum(),
            'numeric_summary': df.describe().to_dict(),
            'categorical_info': {}
        }
        
        # Categorical column analysis
        categorical_cols = df.select_dtypes(include=['object']).columns
        for col in categorical_cols:
            exploration['categorical_info'][col] = {
                'unique_count': df[col].nunique(),
                'top_values': df[col].value_counts().head().to_dict()
            }
        
        return exploration
    
    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Clean and prepare data"""
        # Remove duplicates
        df = df.drop_duplicates()
        
        # Handle outliers for numeric columns
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        for col in numeric_columns:
            if col != self.target_column:
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                # Cap outliers instead of removing
                df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)
        
        # Remove rows with missing target values
        df = df.dropna(subset=[self.target_column])
        
        logger.info(f"Data after cleaning: {df.shape}")
        return df
    
    def create_preprocessor(self, df: pd.DataFrame):
        """Create sklearn preprocessing pipeline"""
        # Identify column types
        numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()
        categorical_features = df.select_dtypes(include=['object']).columns.tolist()
        
        # Remove target column from features
        if self.target_column in numeric_features:
            numeric_features.remove(self.target_column)
        if self.target_column in categorical_features:
            categorical_features.remove(self.target_column)
        
        self.feature_columns = numeric_features + categorical_features
        
        # Create preprocessing pipelines
        numeric_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler())
        ])
        
        categorical_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
        ])
        
        # Combine preprocessing steps
        self.preprocessor = ColumnTransformer(
            transformers=[
                ('num', numeric_transformer, numeric_features),
                ('cat', categorical_transformer, categorical_features)
            ]
        )
        
        # Handle target encoding for classification
        if self.config.get('problem_type') == 'classification':
            self.label_encoder = LabelEncoder()
    
    def fit_transform(self, df: pd.DataFrame):
        """Fit preprocessor and transform data"""
        # Prepare features and target
        X = df[self.feature_columns]
        y = df[self.target_column]
        
        # Transform features
        X_transformed = self.preprocessor.fit_transform(X)
        
        # Transform target for classification
        if self.label_encoder:
            y_transformed = self.label_encoder.fit_transform(y)
        else:
            y_transformed = y.values
        
        return X_transformed, y_transformed
    
    def transform(self, df: pd.DataFrame):
        """Transform new data using fitted preprocessor"""
        X = df[self.feature_columns]
        X_transformed = self.preprocessor.transform(X)
        
        if self.target_column in df.columns:
            y = df[self.target_column]
            if self.label_encoder:
                y_transformed = self.label_encoder.transform(y)
            else:
                y_transformed = y.values
            return X_transformed, y_transformed
        
        return X_transformed
    
    def save_preprocessor(self, path: str):
        """Save preprocessor and encoders"""
        preprocessor_data = {
            'preprocessor': self.preprocessor,
            'label_encoder': self.label_encoder,
            'feature_columns': self.feature_columns,
            'config': self.config
        }
        joblib.dump(preprocessor_data, path)
        logger.info(f"Preprocessor saved to {path}")
    
    def load_preprocessor(self, path: str):
        """Load preprocessor and encoders"""
        preprocessor_data = joblib.load(path)
        self.preprocessor = preprocessor_data['preprocessor']
        self.label_encoder = preprocessor_data['label_encoder']
        self.feature_columns = preprocessor_data['feature_columns']
        self.config = preprocessor_data['config']
        logger.info(f"Preprocessor loaded from {path}")

def prepare_data(config: dict):
    """Main data preparation function"""
    preprocessor = DataPreprocessor(config)
    
    # Load and explore data
    df = preprocessor.load_data(config['data_path'])
    exploration = preprocessor.explore_data(df)
    logger.info(f"Data exploration: {exploration}")
    
    # Clean data
    df_clean = preprocessor.clean_data(df)
    
    # Create and fit preprocessor
    preprocessor.create_preprocessor(df_clean)
    X, y = preprocessor.fit_transform(df_clean)
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, 
        test_size=config.get('test_size', 0.2),
        random_state=config.get('random_state', 42),
        stratify=y if config.get('problem_type') == 'classification' else None
    )
    
    # Save preprocessor
    preprocessor.save_preprocessor(config.get('preprocessor_path', 'preprocessor.joblib'))
    
    return {
        'X_train': X_train, 'X_test': X_test,
        'y_train': y_train, 'y_test': y_test,
        'preprocessor': preprocessor,
        'exploration': exploration
    }
```

### Model Development and Training

```python
# model_trainer.py
import tensorflow as tf
import torch
import torch.nn as nn
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.svm import SVC, SVR
from sklearn.metrics import classification_report, mean_squared_error, r2_score
import mlflow
import mlflow.tensorflow
import mlflow.pytorch
import mlflow.sklearn
from typing import Any, Dict, Tuple
import numpy as np

class ModelTrainer:
    def __init__(self, config: dict):
        self.config = config
        self.model = None
        self.history = None
        
        # Initialize MLflow
        mlflow.set_experiment(config.get('experiment_name', 'ml_experiment'))
    
    def create_tensorflow_model(self, input_shape: int, num_classes: int = None) -> tf.keras.Model:
        """Create TensorFlow/Keras model"""
        model = tf.keras.Sequential([
            tf.keras.layers.Dense(128, activation='relu', input_shape=(input_shape,)),
            tf.keras.layers.Dropout(0.3),
            tf.keras.layers.Dense(64, activation='relu'),
            tf.keras.layers.Dropout(0.3),
            tf.keras.layers.Dense(32, activation='relu'),
        ])
        
        if self.config['problem_type'] == 'classification':
            if num_classes > 2:
                model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))
                model.compile(
                    optimizer='adam',
                    loss='sparse_categorical_crossentropy',
                    metrics=['accuracy']
                )
            else:
                model.add(tf.keras.layers.Dense(1, activation='sigmoid'))
                model.compile(
                    optimizer='adam',
                    loss='binary_crossentropy',
                    metrics=['accuracy']
                )
        else:  # regression
            model.add(tf.keras.layers.Dense(1))
            model.compile(
                optimizer='adam',
                loss='mse',
                metrics=['mae']
            )
        
        return model
    
    def create_pytorch_model(self, input_size: int, num_classes: int = None) -> nn.Module:
        """Create PyTorch model"""
        class NeuralNetwork(nn.Module):
            def __init__(self, input_size, num_classes=None):
                super().__init__()
                self.fc1 = nn.Linear(input_size, 128)
                self.fc2 = nn.Linear(128, 64)
                self.fc3 = nn.Linear(64, 32)
                
                if self.config['problem_type'] == 'classification':
                    self.output = nn.Linear(32, num_classes if num_classes > 2 else 1)
                else:
                    self.output = nn.Linear(32, 1)
                
                self.dropout = nn.Dropout(0.3)
                self.relu = nn.ReLU()
            
            def forward(self, x):
                x = self.relu(self.fc1(x))
                x = self.dropout(x)
                x = self.relu(self.fc2(x))
                x = self.dropout(x)
                x = self.relu(self.fc3(x))
                x = self.output(x)
                return x
        
        return NeuralNetwork(input_size, num_classes)
    
    def create_sklearn_model(self):
        """Create scikit-learn model"""
        if self.config['problem_type'] == 'classification':
            if self.config.get('model_type') == 'random_forest':
                return RandomForestClassifier(
                    n_estimators=100,
                    random_state=42,
                    n_jobs=-1
                )
            elif self.config.get('model_type') == 'logistic_regression':
                return LogisticRegression(random_state=42, max_iter=1000)
            elif self.config.get('model_type') == 'svm':
                return SVC(random_state=42)
        else:  # regression
            if self.config.get('model_type') == 'random_forest':
                return RandomForestRegressor(
                    n_estimators=100,
                    random_state=42,
                    n_jobs=-1
                )
            elif self.config.get('model_type') == 'linear_regression':
                return LinearRegression()
            elif self.config.get('model_type') == 'svm':
                return SVR()
    
    def train_model(self, X_train: np.ndarray, y_train: np.ndarray, 
                   X_val: np.ndarray = None, y_val: np.ndarray = None) -> Any:
        """Train the model based on framework"""
        
        with mlflow.start_run():
            # Log parameters
            mlflow.log_params(self.config)
            
            framework = self.config.get('framework', 'sklearn')
            
            if framework == 'tensorflow':
                self.model = self.create_tensorflow_model(
                    X_train.shape[1], 
                    len(np.unique(y_train)) if self.config['problem_type'] == 'classification' else None
                )
                
                callbacks = [
                    tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),
                    tf.keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.5)
                ]
                
                validation_data = (X_val, y_val) if X_val is not None else None
                
                self.history = self.model.fit(
                    X_train, y_train,
                    epochs=self.config.get('epochs', 100),
                    batch_size=self.config.get('batch_size', 32),
                    validation_data=validation_data,
                    callbacks=callbacks,
                    verbose=1
                )
                
                # Log model
                mlflow.tensorflow.log_model(self.model, "model")
                
            elif framework == 'pytorch':
                # PyTorch training implementation
                self.model = self.create_pytorch_model(
                    X_train.shape[1],
                    len(np.unique(y_train)) if self.config['problem_type'] == 'classification' else None
                )
                
                # Training loop would go here
                self._train_pytorch_model(X_train, y_train, X_val, y_val)
                
                # Log model
                mlflow.pytorch.log_model(self.model, "model")
                
            else:  # sklearn
                self.model = self.create_sklearn_model()
                self.model.fit(X_train, y_train)
                
                # Log model
                mlflow.sklearn.log_model(self.model, "model")
            
            return self.model
    
    def _train_pytorch_model(self, X_train, y_train, X_val, y_val):
        """PyTorch specific training loop"""
        # Convert to tensors
        X_train_tensor = torch.FloatTensor(X_train)
        y_train_tensor = torch.LongTensor(y_train) if self.config['problem_type'] == 'classification' else torch.FloatTensor(y_train)
        
        # Define loss and optimizer
        if self.config['problem_type'] == 'classification':
            criterion = nn.CrossEntropyLoss() if len(np.unique(y_train)) > 2 else nn.BCEWithLogitsLoss()
        else:
            criterion = nn.MSELoss()
        
        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)
        
        # Training loop
        epochs = self.config.get('epochs', 100)
        batch_size = self.config.get('batch_size', 32)
        
        for epoch in range(epochs):
            self.model.train()
            total_loss = 0
            
            # Mini-batch training
            for i in range(0, len(X_train_tensor), batch_size):
                batch_X = X_train_tensor[i:i+batch_size]
                batch_y = y_train_tensor[i:i+batch_size]
                
                optimizer.zero_grad()
                outputs = self.model(batch_X)
                loss = criterion(outputs.squeeze(), batch_y)
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
            
            if epoch % 10 == 0:
                print(f'Epoch {epoch}, Loss: {total_loss/len(X_train_tensor):.4f}')
    
    def evaluate_model(self, X_test: np.ndarray, y_test: np.ndarray) -> Dict:
        """Evaluate model performance"""
        if self.config.get('framework') == 'tensorflow':
            predictions = self.model.predict(X_test)
            if self.config['problem_type'] == 'classification':
                y_pred = (predictions > 0.5).astype(int) if predictions.shape[1] == 1 else np.argmax(predictions, axis=1)
            else:
                y_pred = predictions.flatten()
        
        elif self.config.get('framework') == 'pytorch':
            self.model.eval()
            with torch.no_grad():
                X_test_tensor = torch.FloatTensor(X_test)
                predictions = self.model(X_test_tensor)
                if self.config['problem_type'] == 'classification':
                    y_pred = torch.argmax(predictions, dim=1).numpy()
                else:
                    y_pred = predictions.squeeze().numpy()
        
        else:  # sklearn
            y_pred = self.model.predict(X_test)
        
        # Calculate metrics
        if self.config['problem_type'] == 'classification':
            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
            
            metrics = {
                'accuracy': accuracy_score(y_test, y_pred),
                'precision': precision_score(y_test, y_pred, average='weighted'),
                'recall': recall_score(y_test, y_pred, average='weighted'),
                'f1_score': f1_score(y_test, y_pred, average='weighted')
            }
            
            # Log metrics
            mlflow.log_metrics(metrics)
            
            # Classification report
            report = classification_report(y_test, y_pred)
            print("Classification Report:")
            print(report)
            
        else:  # regression
            metrics = {
                'mse': mean_squared_error(y_test, y_pred),
                'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),
                'r2_score': r2_score(y_test, y_pred)
            }
            
            # Log metrics
            mlflow.log_metrics(metrics)
            
            print(f"MSE: {metrics['mse']:.4f}")
            print(f"RMSE: {metrics['rmse']:.4f}")
            print(f"RÂ² Score: {metrics['r2_score']:.4f}")
        
        return metrics
    
    def save_model(self, path: str):
        """Save trained model"""
        if self.config.get('framework') == 'tensorflow':
            self.model.save(path)
        elif self.config.get('framework') == 'pytorch':
            torch.save(self.model.state_dict(), path)
        else:  # sklearn
            import joblib
            joblib.dump(self.model, path)
        
        print(f"Model saved to {path}")
```

### Model Deployment and Serving

```python
# model_server.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import numpy as np
import joblib
import tensorflow as tf
import torch
import uvicorn
from typing import List, Dict, Any
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="ML Model API", version="1.0.0")

class PredictionRequest(BaseModel):
    features: List[List[float]]
    model_version: str = "latest"

class PredictionResponse(BaseModel):
    predictions: List[float]
    model_version: str
    confidence_scores: List[float] = None

class ModelServer:
    def __init__(self):
        self.models = {}
        self.preprocessors = {}
        self.config = {}
        
    def load_model(self, model_path: str, preprocessor_path: str, 
                   model_name: str, framework: str):
        """Load model and preprocessor"""
        try:
            # Load preprocessor
            preprocessor_data = joblib.load(preprocessor_path)
            self.preprocessors[model_name] = preprocessor_data
            
            # Load model based on framework
            if framework == 'tensorflow':
                model = tf.keras.models.load_model(model_path)
            elif framework == 'pytorch':
                # Load PyTorch model (assuming state dict)
                model = torch.load(model_path)
            else:  # sklearn
                model = joblib.load(model_path)
            
            self.models[model_name] = model
            self.config[model_name] = {
                'framework': framework,
                'problem_type': preprocessor_data['config']['problem_type']
            }
            
            logger.info(f"Model {model_name} loaded successfully")
            
        except Exception as e:
            logger.error(f"Error loading model {model_name}: {e}")
            raise
    
    def predict(self, model_name: str, features: np.ndarray) -> Dict[str, Any]:
        """Make predictions using loaded model"""
        if model_name not in self.models:
            raise ValueError(f"Model {model_name} not found")
        
        model = self.models[model_name]
        preprocessor = self.preprocessors[model_name]['preprocessor']
        config = self.config[model_name]
        
        try:
            # Preprocess features
            features_processed = preprocessor.transform(features)
            
            # Make predictions based on framework
            if config['framework'] == 'tensorflow':
                predictions = model.predict(features_processed)
                if config['problem_type'] == 'classification':
                    if predictions.shape[1] == 1:
                        # Binary classification
                        confidence_scores = predictions.flatten()
                        predictions = (predictions > 0.5).astype(int).flatten()
                    else:
                        # Multi-class classification
                        confidence_scores = np.max(predictions, axis=1)
                        predictions = np.argmax(predictions, axis=1)
                else:
                    # Regression
                    predictions = predictions.flatten()
                    confidence_scores = None
            
            elif config['framework'] == 'pytorch':
                model.eval()
                with torch.no_grad():
                    features_tensor = torch.FloatTensor(features_processed)
                    outputs = model(features_tensor)
                    
                    if config['problem_type'] == 'classification':
                        probabilities = torch.softmax(outputs, dim=1)
                        confidence_scores = torch.max(probabilities, dim=1)[0].numpy()
                        predictions = torch.argmax(outputs, dim=1).numpy()
                    else:
                        predictions = outputs.squeeze().numpy()
                        confidence_scores = None
            
            else:  # sklearn
                predictions = model.predict(features_processed)
                
                # Get confidence scores for classification
                if config['problem_type'] == 'classification' and hasattr(model, 'predict_proba'):
                    probabilities = model.predict_proba(features_processed)
                    confidence_scores = np.max(probabilities, axis=1)
                else:
                    confidence_scores = None
            
            return {
                'predictions': predictions.tolist(),
                'confidence_scores': confidence_scores.tolist() if confidence_scores is not None else None
            }
            
        except Exception as e:
            logger.error(f"Error making predictions: {e}")
            raise

# Initialize model server
model_server = ModelServer()

# Load models on startup (configure as needed)
@app.on_event("startup")
async def load_models():
    """Load models on server startup"""
    try:
        # Load your trained models here
        model_server.load_model(
            model_path="[MODEL_PATH]",
            preprocessor_path="[PREPROCESSOR_PATH]",
            model_name="[MODEL_NAME]",
            framework="[FRAMEWORK]"
        )
        logger.info("All models loaded successfully")
    except Exception as e:
        logger.error(f"Error loading models: {e}")

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "healthy", "models_loaded": list(model_server.models.keys())}

@app.post("/predict/{model_name}", response_model=PredictionResponse)
async def predict(model_name: str, request: PredictionRequest):
    """Make predictions using specified model"""
    try:
        # Convert features to numpy array
        features = np.array(request.features)
        
        # Make predictions
        result = model_server.predict(model_name, features)
        
        return PredictionResponse(
            predictions=result['predictions'],
            confidence_scores=result['confidence_scores'],
            model_version=request.model_version
        )
        
    except Exception as e:
        logger.error(f"Prediction error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/models")
async def list_models():
    """List available models"""
    return {
        "available_models": list(model_server.models.keys()),
        "model_configs": model_server.config
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### Model Monitoring and Evaluation

```python
# model_monitor.py
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score
import mlflow
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

class ModelMonitor:
    def __init__(self, model_name: str, tracking_uri: str = None):
        self.model_name = model_name
        if tracking_uri:
            mlflow.set_tracking_uri(tracking_uri)
        
        self.predictions_log = []
        self.performance_history = []
    
    def log_prediction(self, features: np.ndarray, prediction: Any, 
                      actual: Any = None, confidence: float = None):
        """Log individual prediction for monitoring"""
        log_entry = {
            'timestamp': datetime.now(),
            'features': features.tolist(),
            'prediction': prediction,
            'actual': actual,
            'confidence': confidence
        }
        self.predictions_log.append(log_entry)
    
    def calculate_drift(self, reference_data: np.ndarray, 
                       current_data: np.ndarray) -> Dict[str, float]:
        """Calculate data drift metrics"""
        drift_metrics = {}
        
        for i in range(reference_data.shape[1]):
            ref_col = reference_data[:, i]
            curr_col = current_data[:, i]
            
            # Statistical tests for drift
            from scipy import stats
            
            # KS test for distribution drift
            ks_stat, ks_p = stats.ks_2samp(ref_col, curr_col)
            drift_metrics[f'feature_{i}_ks_stat'] = ks_stat
            drift_metrics[f'feature_{i}_ks_p_value'] = ks_p
            
            # Mean and standard deviation drift
            ref_mean, ref_std = np.mean(ref_col), np.std(ref_col)
            curr_mean, curr_std = np.mean(curr_col), np.std(curr_col)
            
            drift_metrics[f'feature_{i}_mean_drift'] = abs(curr_mean - ref_mean) / ref_std
            drift_metrics[f'feature_{i}_std_drift'] = abs(curr_std - ref_std) / ref_std
        
        return drift_metrics
    
    def evaluate_performance(self, y_true: np.ndarray, y_pred: np.ndarray, 
                           problem_type: str) -> Dict[str, float]:
        """Evaluate model performance"""
        if problem_type == 'classification':
            metrics = {
                'accuracy': accuracy_score(y_true, y_pred),
                'precision': precision_score(y_true, y_pred, average='weighted'),
                'recall': recall_score(y_true, y_pred, average='weighted')
            }
        else:  # regression
            from sklearn.metrics import mean_squared_error, r2_score
            metrics = {
                'mse': mean_squared_error(y_true, y_pred),
                'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),
                'r2_score': r2_score(y_true, y_pred)
            }
        
        # Log to MLflow
        with mlflow.start_run():
            mlflow.log_metrics(metrics)
        
        self.performance_history.append({
            'timestamp': datetime.now(),
            'metrics': metrics
        })
        
        return metrics
    
    def generate_monitoring_report(self) -> Dict:
        """Generate comprehensive monitoring report"""
        if not self.predictions_log:
            return {"error": "No predictions logged"}
        
        df = pd.DataFrame(self.predictions_log)
        
        report = {
            'model_name': self.model_name,
            'report_date': datetime.now().isoformat(),
            'total_predictions': len(df),
            'date_range': {
                'start': df['timestamp'].min().isoformat(),
                'end': df['timestamp'].max().isoformat()
            }
        }
        
        # Prediction distribution
        if 'confidence' in df.columns and df['confidence'].notna().any():
            report['confidence_stats'] = {
                'mean': df['confidence'].mean(),
                'std': df['confidence'].std(),
                'min': df['confidence'].min(),
                'max': df['confidence'].max()
            }
        
        # Performance metrics if actuals are available
        if 'actual' in df.columns and df['actual'].notna().any():
            actuals = df[df['actual'].notna()]
            if len(actuals) > 0:
                accuracy = accuracy_score(actuals['actual'], actuals['prediction'])
                report['recent_accuracy'] = accuracy
        
        return report

# Usage example
def train_and_monitor_model():
    """Complete ML pipeline with monitoring"""
    
    # Configuration
    config = {
        'data_path': 'data.csv',
        'target_column': 'target',
        'problem_type': 'classification',  # or 'regression'
        'framework': 'sklearn',  # 'tensorflow', 'pytorch', 'sklearn'
        'model_type': 'random_forest',
        'test_size': 0.2,
        'random_state': 42,
        'experiment_name': 'ml_experiment'
    }
    
    # Data preparation
    from data_pipeline import prepare_data
    data = prepare_data(config)
    
    # Model training
    trainer = ModelTrainer(config)
    model = trainer.train_model(data['X_train'], data['y_train'])
    
    # Model evaluation
    metrics = trainer.evaluate_model(data['X_test'], data['y_test'])
    
    # Save model
    trainer.save_model('model.pkl')
    
    # Initialize monitoring
    monitor = ModelMonitor('my_model')
    
    print("Model training and setup complete!")
    print(f"Test metrics: {metrics}")
    
    return model, monitor

if __name__ == "__main__":
    model, monitor = train_and_monitor_model()
```

## Success Criteria
- Model achieves target performance metrics
- Data preprocessing pipeline is robust
- Model deployment is production-ready
- Monitoring and drift detection implemented
- MLflow tracking configured correctly
- API endpoints working with proper error handling